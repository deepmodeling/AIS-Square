# DeePMD-kit Handson Tutorial(v2.1.0)
Last updated on 20230215
## Table of Contents
* Get tutorial data via git
* General Introduction
* Installation
* Data preparation
* Prepare input script
* Train a model
* Freeze and Compress a model
* Test a model
* Run MD with LAMMPS
* Model Inference with Python interface

This handson tutorial is supported by [Bohrium](https://bohrium.dp.tech). You can refer to [Bohrium's Document](https://bohrium-doc.dp.tech/#/docs/DP-GEN ) to start a DP-GEN node for following experiments.

## Get tutorial data via git
After entering the DP-GEN node,
```
$ cd /data
$ git clone https://gitee.com/deepmodeling/colombo-academy-tutorials.git
$ cd /data/colombo-academy-tutorials

```
	
## General Introduction
This tutorial will introduce you to the basic usage of the DeePMD-kit, taking a gas phase methane molecule as an example. [DeePMD-kit's document](https://docs.deepmodeling.org/projects/deepmd/en/master/index.html 
) is recommended as the complete reference.

The DP model is generated using the DeePMD-kit package (v2.1.0). The training data is converted into the format of DeePMD-kit using a tool named dpdata (v0.2.5). It needs to be noted that dpdata only works with Python 3.5 and later versions. The MD simulations are carried out using LAMMPS (29 Sep 2021) integrated with DeePMD-kit. Details of dpdata and DeePMD-kit installation and execution of can be found in [the DeepModeling official GitHub site](https://github.com/deepmodeling). 

We've prepared initial data for CH4 for you. We've put them in the folder DeePMD-kit

The folder structure of this tutorial is like this:
```
$ cd DeePMD-kit
$ ls
00.data 01.train 02.lmp
```
1. The folder 00.data contains the data obtained from CP2K.
2. The folder 01.train contains an input script to train a model with DeePMD-kit
3. The folder 02.lmp contains LAMMPS files for molecular dynamics simulation


## Installation

(On Bohrium, DeePMD-kit is already installed, so you can skip this section)
For convenience, we download DeePMD-kit's offline package in advance. You can get it via such process.
```
$ wget https://dp-filetrans.oss-cn-beijing.aliyuncs.com/zyz/deepmd-kit-2.1.0-cpu-Linux-x86_64.sh

# Maybe there will be incomplete information due to too long line, 
# try deepmd-kit-2.1.0-cpu-Linux-x86_64.sh 

$ bash deepmd-kit-2.1.0-cpu-Linux-x86_64.sh -b -p /root/local/deepmd-kit-2.1.0-cpu 

$ export PATH="/root/local/deepmd-kit-2.1.0-cpu/bin/:$PATH"

$ dp

```

## Data preparation
The training data of the DeePMD-kit contains the atom type, the simulation box, the atom coordinate, the atom force, the system energy, and the virial. A snapshot of a molecular system that has this information is called a frame. A system of data includes many frames that share the same number of atoms and atom types. For example, a molecular dynamics trajectory can be converted into a system of data, with each time step corresponding to a frame in the system.

The DeePMD-kit adopts a compressed data format. All training data should first be converted into this format and can then be used by DeePMD-kit. The data format is explained in detail in the DeePMD-kit manual that can be found in [the DeePMD-kit official Github site](http://www.github.com/deepmodeling/deepmd-kit) .

We provide a convenient tool named dpdata for converting the data produced by CP2K, Gaussian, Quantum-Espresso, ABACUS, and LAMMPS into the compressed format of DeePMD-kit.

As an example, go to the data folder:
```
$ cd 00.data
$ ls 
aimd abacus_md readData_CP2K.py readData_ABACUS.py
```
The 'aimd' folder was produced by an ab-initio molecular dynamics (AIMD) simulation of a gas phase methane molecule using CP2K. 
The 'abacus_md' folder was produced by an ab-initio molecular dynamics (AIMD) simulation of a gas phase methane molecule using ABACUS. `readData_CP2K.py` can read the CP2K data, and change it into DeePMD format. `readData_ABACUS.py` can read the ABACUS data, and change it into DeePMD format. 

```
$ python readData_CP2K.py
```
Now let's have a look at `readData_CP2K.py`.On the screen, you can see that the 'aimd' folder contains 201 frames of data. We randomly pick 40 frames as validation data and the rest as training data. 
```python
import dpdata 
import numpy as np
data = dpdata.LabeledSystem('aimd', fmt = 'cp2k/aimd_output') 
print(len(data))

index_validation = np.random.choice(201,size=40,replace=False)     
# random choose 40 index for validation_data
index_training = list(set(range(201))-set(index_validation))
# other indexes are training_data
data_training = data.sub_system(index_training)
data_validation = data.sub_system(index_validation)
data_training.to_deepmd_npy('training_data')               
# all training data put into directory:"training_data" 
data_validation.to_deepmd_npy('validation_data')          
# all validation data put into directory:"validation_data"
print(len(data_training)) 
print(len(data_validation)) 
```
The python script import a system of data from the 'aimd' folder (with format cp2k/aimd_output ), and then dump it into the compressed format (numpy compressed arrays). The data in DeePMD-kit format is stored in the folder 00.data. Let's have a look:
```
$ ls 
aimd readData.py training_data validation_data
```
The directories "training_data" and "validation_data" have similar structure, so we just explain "training_data":
```
$ ls training_data
set.000 type.raw type_map.raw
```
1. set.000: is a directory, contains data in compressed format (numpy compressed arrays). 
2. type.raw: is a file, contains types of atoms(Represented in integer)
3. type_map.raw: is a file,  contains the atom type names.

Lets have a look at `type.raw`:
```
$ cat training_data/type.raw 
0 
0 
0 
0 
1
```
This tells us there are 5 atoms in this example, 4 atoms represented by type "0", and 1 atom represented by type "1".
Sometimes one needs to map the integer types to atom name. The mapping can be given by the file `type_map.raw`
```
$ cat training_data/type_map.raw 
H 
C
```
This tells us the type "0" is named by "H", and the type "1" is named by "C".

More detailed doc about Data conversion can be found [here](https://docs.deepmodeling.org/projects/deepmd/en/master/data/data-conv.html)

## Training
### Prepare input script 
Once the data preparation is done, we can go on with training. Now go to the training directory
```
$ cd ../01.train
$ ls 
input.json
```
where input.json gives you an example training script. The options are explained in detail in the DeePMD-kit manual, so they are not comprehensively explained. 

In the model section, the parameters of embedding and fitting networks are specified.
```json
"model":{
    "type_map":    ["H", "C"],                 # the name of each type of atom
    "descriptor":{
        "type":            "se_e2_a",          # full relative coordinates are used
        "rcut":            6.00,               # cut-off radius
        "rcut_smth":       0.50,               # where the smoothing starts
        "sel":             [4, 1],             # the maximum number of type i atoms in the cut-off radius
        "neuron":          [10, 20, 40],       # size of the embedding neural network
        "resnet_dt":       false,
        "axis_neuron":     4,                  # the size of the submatrix of G (embedding matrix)
        "seed":            1,
        "_comment":        "that's all"
        },
    "fitting_net":{
        "neuron":          [100, 100, 100],    # size of the fitting neural network
        "resnet_dt":       true,
        "seed":            1,
        "_comment":        "that's all"
    },
    "_comment":    "that's all"'
},
```
The `se_e2_a` descriptor is used to train the DP model. The item neurons set the size of the descriptors and fitting network to [10, 20, 40] and [100, 100, 100], respectively. The components in local environment to smoothly go to zero from 0.5 to 6 Ã….

The following are the parameters that specify the learning rate and loss function.
```json
    "learning_rate" :{
        "type":                "exp",
        "decay_steps":         100,
        "start_lr":            0.001,    
        "stop_lr":             3.51e-8,
        "_comment":            "that's all"
    },
    "loss" :{
        "type":                "ener",
        "start_pref_e":        0.02,
        "limit_pref_e":        1,
        "start_pref_f":        1000,
        "limit_pref_f":        1,
        "start_pref_v":        0,
        "limit_pref_v":        0,
        "_comment":            "that's all"
    },
```
In the loss function, `pref_e` increases from 0.02 to 1, and `pref_f` decreases from 1000 to 1  progressively, which means that the force term dominates at the beginning, while energy and virial terms become important at the end. This strategy is very effective and reduces the total training time. `pref_v` is set to 0 , indicating that no virial data are included in the training process. The starting learning rate, stop learning rate, and decay steps are set to 0.001, 3.51e-8, and 5000, respectively. The model is trained for 400,000 steps.

The training parameters are given in the following
```json
    "training" : {
        "training_data": {
            "systems":            ["../00.data/training_data"],     
            "batch_size":         "auto",                       
            "_comment":           "that's all"
        },
        "validation_data":{
            "systems":            ["../00.data/validation_data/"],
            "batch_size":         "auto",               
            "numb_btch":          1,
            "_comment":           "that's all"
        },
        "numb_steps":             20000,                           
        "seed":                   10,
        "disp_file":              "lcurve.out",
        "disp_freq":              1000,
        "save_freq":              10000,
        },
```
### Train a model
After the training script is prepared, we can start the training with DeePMD-kit by simply running
```
$ dp train input.json 
```
On the screen, you see the information of the data system(s)
```
DEEPMD INFO    -----------------------------------------------------------------
DEEPMD INFO    ---Summary of DataSystem: training     ----------------------------------
DEEPMD INFO    found 1 system(s):
DEEPMD INFO                                 system  natoms  bch_sz   n_bch   prob  pbc
DEEPMD INFO               ../00.data/training_data       5       7      23  1.000    T
DEEPMD INFO    -------------------------------------------------------------------------
DEEPMD INFO    ---Summary of DataSystem: validation   ----------------------------------
DEEPMD INFO    found 1 system(s):
DEEPMD INFO                                 system  natoms  bch_sz   n_bch   prob  pbc
DEEPMD INFO             ../00.data/validation_data       5       7       5  1.000    T
DEEPMD INFO    -------------------------------------------------------------------------
```
and the starting and final learning rate of this training
```
DEEPMD INFO    start training at lr 1.00e-03 (== 1.00e-03), decay_step 100, decay_rate 0.950006, final lr will be 3.51e-08
```
If everything works fine, you will see, on the screen, information printed every 1000 steps, like
```
DEEPMD INFO    batch    1000 training time 10.83 s, testing time 0.01 s
DEEPMD INFO    batch    2000 training time 8.39 s, testing time 0.01 s
DEEPMD INFO    batch    3000 training time 8.38 s, testing time 0.01 s
DEEPMD INFO    batch    4000 training time 8.50 s, testing time 0.01 s
DEEPMD INFO    batch    5000 training time 8.48 s, testing time 0.01 s
DEEPMD INFO    batch    6000 training time 8.24 s, testing time 0.01 s
DEEPMD INFO    batch    7000 training time 8.30 s, testing time 0.01 s
DEEPMD INFO    batch    8000 training time 8.26 s, testing time 0.01 s
DEEPMD INFO    batch    9000 training time 8.18 s, testing time 0.01 s
DEEPMD INFO    batch   10000 training time 8.30 s, testing time 0.01 s
DEEPMD INFO    saved checkpoint model.ckpt
```
They present the training and testing time counts. At the end of the 10000th batch, the model is saved in Tensorflow's checkpoint file `model.ckpt`. At the same time, the training and testing errors are presented in file `lcurve.out`. 
The file contains 8 columns, form left to right, are the training step, the validation loss, training loss, root mean square (RMS) validation error of energy, RMS training error of energy, RMS validation error of force, RMS training error of force and the learning rate. The RMS error (RMSE) of the energy is normalized by number of atoms in the system. 
```
head -n 2 lcurve.out
#  step      rmse_val    rmse_trn    rmse_e_val  rmse_e_trn    rmse_f_val  rmse_f_trn         lr
      0      2.82e+01    2.66e+01      7.27e-01    7.11e-01      8.92e-01    8.43e-01    1.0e-03
```
and
```
$ tail -n 2 lcurve.out
  19000      1.74e-02    1.78e-02      2.41e-04    3.71e-04      1.69e-02    1.73e-02    5.9e-08
  20000      2.69e-02    1.51e-02      6.45e-04    2.58e-04      2.64e-02    1.48e-02    3.5e-08
```
Volumes 4, 5 and 6, 7 present energy and force training and testing errors, respectively. 


### Freeze and Compress a model
At the end of the training, the model parameters saved in TensorFlow's checkpoint file should be frozen as a model file that is usually ended with extension .pb. Simply execute
```
$ dp freeze -o graph.pb
DEEPMD INFO    Restoring parameters from ./model.ckpt-20000
DEEPMD INFO    1150 ops in the final graph
```
and it will output a model file named `graph.pb` in the current directory. 
The compressed DP model typically speed up DP-based calculations by an order of magnitude faster, and consume an order of magnitude less memory. The `graph.pb` can be compressed in the following way:
```
$ dp compress -i graph.pb -o graph-compress.pb
DEEPMD INFO    stage 1: compress the model
DEEPMD INFO    built lr
DEEPMD INFO    built network
DEEPMD INFO    built training
DEEPMD INFO    initialize model from scratch
DEEPMD INFO    finished compressing
DEEPMD INFO    
DEEPMD INFO    stage 2: freeze the model
DEEPMD INFO    Restoring parameters from model-compression/model.ckpt
DEEPMD INFO    840 ops in the final graph
```
and it will output a model file named `graph-compress.pb`.

### Test a model
We can check the quality of the trained model by running
```
$ dp test -m graph-compress.pb -s ../00.data/validation_data -n 40 -d results
```
On the screen you see the information of the prediction errors of validation data
```
DEEPMD INFO    # ---------------output of dp test--------------- 
DEEPMD INFO    # testing system : ../00.data/validation_data
DEEPMD INFO    # number of test data : 40 
DEEPMD INFO    Energy RMSE        : 1.136239e-04 eV
DEEPMD INFO    Energy RMSE/Natoms : 2.272477e-05 eV
DEEPMD INFO    Force  RMSE        : 2.193129e-03 eV/A
DEEPMD INFO    Virial RMSE        : 8.698923e-01 eV
DEEPMD INFO    Virial RMSE/Natoms : 1.739785e-01 eV
DEEPMD INFO    # ----------------------------------------------- 
```
and it will output files named results.e.out and results.f.out in the current directory.

## Run MD with LAMMPS

Now let's switch to the lammps directory to check the necessary input files for running DeePMD with LAMMPS.
```
$ cd ../02.lmp
```
Firstly, we soft-link the output model in the training directory to the current directory
```
$ cp ../01.train/graph*.pb ./;
```
Then we have four files
```
$ ls
conf.lmp  graph.pb  graph-compress.pb  in.lammps
```
where `conf.lmp` gives the initial configuration of a gas phase methane MD simulation, and the file `in.lammps` is the lammps input script. One may check in.lammps and finds that it is a rather standard LAMMPS input file for a MD simulation, with only two exception lines:
```
pair_style  deepmd graph.pb
pair_coeff  * *
```
where the pair style deepmd is invoked and the model file `graph.pb` is provided, which means the atomic interaction will be computed by the DP model that is stored in the file `graph.pb`. You can also replace the model file with `graph-compressed.pb` and compare the efficiency.

One may execute lammps in the standard way
```
$ lmp  -i  in.lammps
```

If you install lammps mannualy, you may execute lammps:
```
$ lmp_mpi -i in.lammps
```

After waiting for a while, the MD simulation finishes, and the log.lammps and ch4.dump files are generated. They store thermodynamic information and the trajectory of the molecule, respectively. 

## Model inference with Python interface
After MD simulation, a trajectory within LAMMPS format will be generated. You can use the deep potential to evalute the energies, forces of these configurations directly with Python interface.

``` python
d=dpdata.System('ch4.dump', fmt = "lammps/dump", type_map = ["H", "C"])
d1 = d.predict(dp = "./graph-compress.pb")
d1["energies"]
```
Now, you've finished this tutorial!